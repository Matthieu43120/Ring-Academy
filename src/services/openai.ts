// Configuration de l'API OpenAI
const OPENAI_PROXY_URL = '/.netlify/functions/openai-proxy';
const OPENAI_AUDIO_URL = '/.netlify/functions/openai-audio';

// Interface pour les param√®tres de g√©n√©ration de r√©ponse IA
interface AIResponseParams {
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  target: string;
  onPartialText?: (text: string) => void;
  onSentenceReadyForAudio?: (sentence: string) => void;
  onTextReady?: (text: string) => void;
}

// Fonction pour g√©n√©rer une r√©ponse IA rapide avec streaming
export async function generateAIResponseFast(params: AIResponseParams): Promise<string> {
  const { messages, target, onPartialText, onSentenceReadyForAudio, onTextReady } = params;
  
  console.log('üöÄ D√©marrage streaming IA...');
  
  try {
    const response = await fetch(OPENAI_PROXY_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        messages,
        target,
        stream: true
      }),
    });

    if (!response.ok) {
      throw new Error(`Erreur HTTP: ${response.status}`);
    }

    return await processStreamingResponse(response, target, onPartialText, onSentenceReadyForAudio, onTextReady);
  } catch (error) {
    console.error('‚ùå Erreur g√©n√©ration IA:', error);
    throw error;
  }
}

// Fonction pour traiter la r√©ponse streaming
async function processStreamingResponse(
  response: Response,
  target: string,
  onPartialText?: (text: string) => void,
  onSentenceReadyForAudio?: (sentence: string) => void,
  onTextReady?: (text: string) => void
): Promise<string> {
  const reader = response.body?.getReader();
  if (!reader) {
    throw new Error('Impossible de lire la r√©ponse streaming');
  }

  const decoder = new TextDecoder();
  let accumulatedText = '';
  let sentenceBuffer = '';
  let hasStartedProcessing = false;

  try {
    while (true) {
      const { done, value } = await reader.read();
      
      if (done) {
        console.log('‚úÖ Streaming termin√©');
        break;
      }

      const chunk = decoder.decode(value, { stream: true });
      console.log('üì¶ Chunk re√ßu:', chunk);

      // Traiter chaque ligne du chunk
      const lines = chunk.split('\n');
      
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          
          if (data === '[DONE]') {
            console.log('üèÅ Signal de fin re√ßu');
            break;
          }

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices?.[0]?.delta?.content || '';
            
            if (content) {
              if (!hasStartedProcessing) {
                hasStartedProcessing = true;
                console.log('üéØ Premier contenu re√ßu, d√©marrage traitement...');
              }

              accumulatedText += content;
              sentenceBuffer += content;

              // Callback pour le texte partiel
              if (onPartialText) {
                onPartialText(accumulatedText);
              }

              // D√©tecter les phrases compl√®tes
              const completeSentences = extractCompleteSentences(sentenceBuffer);
              
              for (const sentence of completeSentences) {
                console.log('üéµ Phrase compl√®te d√©tect√©e:', sentence);
                
                if (onSentenceReadyForAudio) {
                  onSentenceReadyForAudio(sentence);
                }
                
                // Retirer la phrase du buffer
                sentenceBuffer = sentenceBuffer.replace(sentence, '').trim();
              }
            }
          } catch (parseError) {
            console.warn('‚ö†Ô∏è Erreur parsing JSON:', parseError);
          }
        }
      }
    }

    // Traiter le reste du buffer s'il y en a
    if (sentenceBuffer.trim()) {
      console.log('üéµ Phrase finale du buffer:', sentenceBuffer);
      if (onSentenceReadyForAudio) {
        onSentenceReadyForAudio(sentenceBuffer.trim());
      }
    }

    const cleanMessage = accumulatedText.trim();
    console.log('‚úÖ Message IA final:', cleanMessage);
    
    // Callback final avec le texte complet
    if (onTextReady && cleanMessage) {
      onTextReady(cleanMessage);
    }

    return cleanMessage;
  } finally {
    reader.releaseLock();
  }
}

// Fonction pour extraire les phrases compl√®tes
function extractCompleteSentences(text: string): string[] {
  const sentences: string[] = [];
  
  // Regex pour d√©tecter les fins de phrases
  const sentenceEndRegex = /[.!?]+\s+/g;
  let lastIndex = 0;
  let match;

  while ((match = sentenceEndRegex.exec(text)) !== null) {
    const sentence = text.slice(lastIndex, match.index + match[0].length).trim();
    if (sentence.length > 5) {
      sentences.push(sentence);
      lastIndex = match.index + match[0].length;
    }
  }

  return sentences;
}

// Fonction pour g√©n√©rer l'audio OpenAI de mani√®re synchrone
export async function generateOpenAIAudioSync(text: string): Promise<ArrayBuffer> {
  console.log('üé§ G√©n√©ration audio pour:', text.substring(0, 50) + '...');
  
  try {
    const response = await fetch(OPENAI_AUDIO_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        text: text,
        voice: 'nova',
        model: 'tts-1'
      }),
    });

    if (!response.ok) {
      throw new Error(`Erreur g√©n√©ration audio: ${response.status}`);
    }

    const audioBuffer = await response.arrayBuffer();
    console.log('‚úÖ Audio g√©n√©r√©, taille:', audioBuffer.byteLength);
    return audioBuffer;
  } catch (error) {
    console.error('‚ùå Erreur g√©n√©ration audio:', error);
    throw error;
  }
}

// Fonction pour jouer l'audio OpenAI directement
export async function playOpenAIAudioDirectly(audioBuffer: ArrayBuffer): Promise<void> {
  return new Promise((resolve, reject) => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      
      audioContext.decodeAudioData(audioBuffer.slice(0), (decodedData) => {
        const source = audioContext.createBufferSource();
        source.buffer = decodedData;
        source.connect(audioContext.destination);
        
        source.onended = () => {
          console.log('üîä Lecture audio termin√©e');
          resolve();
        };
        
        source.start(0);
        console.log('üîä D√©but lecture audio');
      }, (error) => {
        console.error('‚ùå Erreur d√©codage audio:', error);
        reject(error);
      });
    } catch (error) {
      console.error('‚ùå Erreur lecture audio:', error);
      reject(error);
    }
  });
}

// Fonction pour g√©n√©rer et jouer un segment audio
export async function generateAndPlaySegmentAudio(text: string): Promise<void> {
  try {
    console.log('üéµ G√©n√©ration et lecture pour:', text.substring(0, 30) + '...');
    const audioBuffer = await generateOpenAIAudioSync(text);
    await playOpenAIAudioDirectly(audioBuffer);
  } catch (error) {
    console.error('‚ùå Erreur g√©n√©ration/lecture segment:', error);
    // Fallback vers la synth√®se vocale du navigateur
    await playTextImmediately(text);
  }
}

// Fonction fallback pour jouer le texte imm√©diatement avec la synth√®se vocale
export async function playTextImmediately(text: string): Promise<void> {
  return new Promise((resolve) => {
    if ('speechSynthesis' in window) {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'fr-FR';
      utterance.rate = 1.1;
      utterance.pitch = 1.0;
      
      utterance.onend = () => {
        console.log('üîä Synth√®se vocale termin√©e');
        resolve();
      };
      
      utterance.onerror = () => {
        console.warn('‚ö†Ô∏è Erreur synth√®se vocale');
        resolve();
      };
      
      speechSynthesis.speak(utterance);
      console.log('üîä D√©but synth√®se vocale');
    } else {
      console.warn('‚ö†Ô∏è Synth√®se vocale non support√©e');
      resolve();
    }
  });
}

// Fonction pour transcrire l'audio (utilis√©e par phoneCallService)
export async function transcribeAudio(audioBlob: Blob): Promise<string> {
  try {
    const formData = new FormData();
    formData.append('audio', audioBlob, 'audio.webm');

    const response = await fetch(OPENAI_AUDIO_URL, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`Erreur transcription: ${response.status}`);
    }

    const result = await response.json();
    return result.text || '';
  } catch (error) {
    console.error('‚ùå Erreur transcription:', error);
    return '';
  }
}